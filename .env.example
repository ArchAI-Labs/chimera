# Example .env for Chimera
# Choose your LLM provider and model
# PROVIDER options: ollama, groq, openai, anthropic, google

# --- LLM Provider Selection ---
PROVIDER=ollama
MODEL=qwen3:8b
MANAGER_MODEL=qwen3:8b
BASE_URL=http://localhost:11434

# --- LLM Configuration ---
TEMPERATURE=0.7
MAX_TOKENS=4096
TIMEOUT=300
# Ollama-specific context window
NUM_CTX=4096

# --- Provider API Keys (uncomment the provider you use) ---
# OpenAI
# OPENAI_API_KEY=<your_openai_api_key>
# Groq
# GROQ_API_KEY=<your_groq_api_key>
# Anthropic
# ANTHROPIC_API_KEY=<your_anthropic_api_key>
# Google Gemini
# GEMINI_API_KEY=<your_gemini_api_key>

# --- Embeddings Configuration ---
CHUNK_SIZE=128
CHUNK_OVERLAP=8
EMBEDDER=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2



# --- Qdrant Configuration ---
# Choose where Qdrant runs: memory | docker | cloud
QDRANT_MODE=memory

# For docker mode:
# QDRANT_URL=http://localhost:6333

# For cloud mode:
# QDRANT_HOST=<your-cloud-hostname>
# QDRANT_API_KEY=<your-cloud-api-key>

# For direct host/port:
# QDRANT_HOST=localhost
# QDRANT_PORT=6333

# Optional collection name used in some tools (default may be 'linkedin_knowledge')
# QDRANT_COLLECTION=crew_knowledge

# --- Output Configuration ---
# Where outputs will be written (relative or absolute path)
OUTPUT_DIR=./output
# Example for Windows absolute path:
# OUTPUT_DIR=C:\Users\your.name\local_path\output

# --- Product Sites ---
# List of product sites to index (comma-separated)
# Your Internal Knowledge Base
PRODUCT_SITES="https://raw.githubusercontent.com/ArchAI-Labs/fastmcp-sonarqube-metrics/refs/heads/main/README.md,https://raw.githubusercontent.com/ArchAI-Labs/code_explainer/refs/heads/main/README.md"