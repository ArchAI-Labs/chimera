# this is an example
# you can use different models
## supported LLM providers:
### OpenAI (e.g., GPT-4, GPT-4-Turbo)
### Google (e.g., Gemini models)
### Anthropic (e.g., Claude)
### Groq (fast AI inference in the cloud)
### Ollama (for local deployment)

# check CrewAI documentation: https://docs.crewai.com/concepts/llms
PROVIDER=google
MODEL=gemini/gemini-2.0-flash
MANAGER_MODEL=gemini/gemini-2.5-flash-preview-04-17
GEMINI_API_KEY=<yourAPIkey>
BASE_URL=

# PROVIDER=anthropic
# MODEL=anthropic/claude-3-sonnet-20240229-v1:0
# ANTHROPIC_API_KEY = <yourAPIkey>
# BASE_URL=

# PROVIDER=openai
# MODEL=gpt-4o-mini
# MANAGER_MODEL = gpt-4o
# OPENAI_API_KEY=<yourAPIkey>
# BASE_URL=

# PROVIDER="groq"
# MODEL=groq/llama-3.3-70b-versatile
# MANAGER_MODEL =
# GROQ_API_KEY=<yourAPIkey>
# BASE_URL=

# PROVIDER="ollama"
# MODEL=ollama/phi3
# MANAGER_MODEL =
# BASE_URL=http://localhost:11434

## LLM CONFIG
TEMPERATURE=0.7
MAX_TOKENS=4096
TIMEOUT=300

## QDRANT
QDRANT_MODE=memory # or cloud or docker
# QDRANT_HOST=xyz-example.eu-central.aws.cloud.qdrant.io # if you use QDRANT_MODE=cloud
# QDRANT_API_KEY=your-api-key # if you use QDRANT_MODE=cloud
# QDRANT_URL=http://localhost:6333 # if you use QDRANT_MODE=docker
EMBEDDER=jinaai/jina-embeddings-v2-base-code
COLLECTION=crew_knowledge

## OUTPUT FOLDERS ##
# the folder where outputs will be saved
OUTPUT_DIR=./your_local_path/output/ #or LOCAL_DIR=C:\Users\user.name\local_path\output\